{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './genre_data'\n",
    "AUDIO_PATH = os.path.join(DATA_PATH, 'audio')\n",
    "META_PATH = os.path.join(DATA_PATH, 'metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = os.path.join(META_PATH,'annotations_final_subsample.csv')\n",
    "\n",
    "# we select the last column (-1) as the index column (= filename)\n",
    "metadata = pd.read_csv(csv_file, index_col=0, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no voice</th>\n",
       "      <th>singer</th>\n",
       "      <th>duet</th>\n",
       "      <th>plucking</th>\n",
       "      <th>hard rock</th>\n",
       "      <th>world</th>\n",
       "      <th>bongos</th>\n",
       "      <th>harpsichord</th>\n",
       "      <th>female singing</th>\n",
       "      <th>clasical</th>\n",
       "      <th>...</th>\n",
       "      <th>female singer</th>\n",
       "      <th>rap</th>\n",
       "      <th>metal</th>\n",
       "      <th>hip hop</th>\n",
       "      <th>quick</th>\n",
       "      <th>water</th>\n",
       "      <th>baroque</th>\n",
       "      <th>women</th>\n",
       "      <th>fiddle</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp3_path</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-01-cyclops-262-291.mp3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-02-all_seeing_eye-175-204.mp3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-03-black-175-204.mp3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-04-confusion_says-88-117.mp3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-05-the_beholder-291-320.mp3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    no voice  singer  duet  \\\n",
       "mp3_path                                                                     \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...         0       0     0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...         0       0     0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...         0       0     0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...         0       0     0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...         0       0     0   \n",
       "\n",
       "                                                    plucking  hard rock  \\\n",
       "mp3_path                                                                  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...         0          0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...         0          0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...         0          0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...         0          0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...         0          0   \n",
       "\n",
       "                                                    world  bongos  \\\n",
       "mp3_path                                                            \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "\n",
       "                                                    harpsichord  \\\n",
       "mp3_path                                                          \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "\n",
       "                                                    female singing  clasical  \\\n",
       "mp3_path                                                                       \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0         0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0         0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0         0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0         0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0         0   \n",
       "\n",
       "                                                     ...     female singer  \\\n",
       "mp3_path                                             ...                     \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...   ...                 0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...   ...                 0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...   ...                 0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...   ...                 0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...   ...                 0   \n",
       "\n",
       "                                                    rap  metal  hip hop  \\\n",
       "mp3_path                                                                  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...    0      0        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...    0      0        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...    0      0        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...    0      0        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...    0      0        0   \n",
       "\n",
       "                                                    quick  water  baroque  \\\n",
       "mp3_path                                                                    \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0      0        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0      0        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0      0        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0      0        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0      0        0   \n",
       "\n",
       "                                                    women  fiddle  english  \n",
       "mp3_path                                                                    \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0        0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0        0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0        0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0        0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0        0  \n",
       "\n",
       "[5 rows x 188 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the unneeded column \"clip_id\"\n",
    "cols = \"clip_id\"\n",
    "metadata.drop(cols,axis=1,inplace=True)\n",
    "\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no voice', 'singer', 'duet', 'plucking', 'hard rock', 'world', 'bongos', 'harpsichord', 'female singing', 'clasical', 'sitar', 'chorus', 'female opera', 'male vocal', 'vocals', 'clarinet', 'heavy', 'silence', 'beats', 'men', 'woodwind', 'funky', 'no strings', 'chimes', 'foreign', 'no piano', 'horns', 'classical', 'female', 'no voices', 'soft rock', 'eerie', 'spacey', 'jazz', 'guitar', 'quiet', 'no beat', 'banjo', 'electric', 'solo', 'violins', 'folk', 'female voice', 'wind', 'happy', 'ambient', 'new age', 'synth', 'funk', 'no singing', 'middle eastern', 'trumpet', 'percussion', 'drum', 'airy', 'voice', 'repetitive', 'birds', 'space', 'strings', 'bass', 'harpsicord', 'medieval', 'male voice', 'girl', 'keyboard', 'acoustic', 'loud', 'classic', 'string', 'drums', 'electronic', 'not classical', 'chanting', 'no violin', 'not rock', 'no guitar', 'organ', 'no vocal', 'talking', 'choral', 'weird', 'opera', 'soprano', 'fast', 'acoustic guitar', 'electric guitar', 'male singer', 'man singing', 'classical guitar', 'country', 'violin', 'electro', 'reggae', 'tribal', 'dark', 'male opera', 'no vocals', 'irish', 'electronica', 'horn', 'operatic', 'arabic', 'lol', 'low', 'instrumental', 'trance', 'chant', 'strange', 'drone', 'synthesizer', 'heavy metal', 'modern', 'disco', 'bells', 'man', 'deep', 'fast beat', 'industrial', 'hard', 'harp', 'no flute', 'jungle', 'pop', 'lute', 'female vocal', 'oboe', 'mellow', 'orchestral', 'viola', 'light', 'echo', 'piano', 'celtic', 'male vocals', 'orchestra', 'eastern', 'old', 'flutes', 'punk', 'spanish', 'sad', 'sax', 'slow', 'male', 'blues', 'vocal', 'indian', 'no singer', 'scary', 'india', 'woman', 'woman singing', 'rock', 'dance', 'piano solo', 'guitars', 'no drums', 'jazzy', 'singing', 'cello', 'calm', 'female vocals', 'voices', 'different', 'techno', 'clapping', 'house', 'monks', 'flute', 'not opera', 'not english', 'oriental', 'beat', 'upbeat', 'soft', 'noise', 'choir', 'female singer', 'rap', 'metal', 'hip hop', 'quick', 'water', 'baroque', 'women', 'fiddle', 'english']\n"
     ]
    }
   ],
   "source": [
    "tags_all = metadata.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres = ['classical', 'rock', 'pop', 'jazz', 'techno'] # 'electronic', ## too little data: , 'reggae', 'metal', 'hip hop']\n",
    "\n",
    "n_genres = len(genres)\n",
    "n_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = metadata[genres].sum(axis=1) == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1168, 5)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_metadata = metadata.loc[idx,genres]\n",
    "genre_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classes needs to be a \"1-hot encoded\" numpy array (which our groundtruth already is! we just convert pandas to numpy)\n",
    "classes = genre_metadata.values\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = genre_metadata.index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classical    604\n",
       "rock         218\n",
       "pop           66\n",
       "jazz          48\n",
       "techno       232\n",
       "dtype: int64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_metadata.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1168"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "def create_spectrograms_new(filelist):\n",
    "    print(\"Reading and processing\", len(filelist), \"audio files\")\n",
    "    q = data_loader.SpectrogramParser({'sample_rate':16000,'window_size':0.015,'window_stride':0.0039,'window':'hamming'},normalize='max_frame')\n",
    "    list_spectrograms = []\n",
    "    for i in tqdm_notebook(range(len(filelist))):\n",
    "        sample = filelist[i]\n",
    "        filepath = os.path.join(AUDIO_PATH, sample)\n",
    "        e = q.parse_audio(filepath)\n",
    "        list_spectrograms.append(e.numpy())\n",
    "    return list_spectrograms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and processing 91517 audio files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c8f1059c0a4711a7fba75b7aefa038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=91517), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at /opt/conda/conda-bld/pytorch_1544202130060/work/aten/src/TH/THGeneral.cpp:201",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-75fffce2bf67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_spectrograms_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-6258e2335f48>\u001b[0m in \u001b[0;36mcreate_spectrograms_new\u001b[0;34m(filelist)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilelist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAUDIO_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mlist_spectrograms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m121\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#list_spectrograms.append(e.numpy())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda_notebooks/MasterThesis/src/data_loader.py\u001b[0m in \u001b[0;36mparse_audio\u001b[0;34m(self, audio_path)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_randomly_augmented_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoiseInjector\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0madd_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda_notebooks/MasterThesis/src/data_loader.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(path, channel)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0msound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0msound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torchaudio-0.2-py3.7-linux-x86_64.egg/torchaudio/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filepath, out, normalization, channels_first, num_frames, offset, signalinfo, encodinginfo, filetype)\u001b[0m\n\u001b[1;32m     83\u001b[0m                                              \u001b[0msignalinfo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                                              \u001b[0mencodinginfo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                                              filetype)\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# normalize if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at /opt/conda/conda-bld/pytorch_1544202130060/work/aten/src/TH/THGeneral.cpp:201"
     ]
    }
   ],
   "source": [
    "data = create_spectrograms_new(filelist)\n",
    "data = np.stack( data, axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we saved the audio spectrograms before, we try to load them\n",
    "load_features = True\n",
    "FEAT_FILE = os.path.join(DATA_PATH, \"spectrograms.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./MagnaTagATune/spectrograms_instrumental.npz'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEAT_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features stored to ./MagnaTagATune/spectrograms_instrumental.npz\n"
     ]
    }
   ],
   "source": [
    "np.savez(FEAT_FILE, data=data, classes=classes, filenames=filelist)\n",
    "print(\"Features stored to \" + FEAT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_features:\n",
    "    if os.path.exists(FEAT_FILE):\n",
    "        with np.load(FEAT_FILE) as npz:\n",
    "            data = npz['data']\n",
    "            filelist = npz['filenames']\n",
    "            classes = npz['classes']\n",
    "        print(\"Loaded features successfully: \" + str(len(filelist)), \"files, dimensions:\", data.shape)\n",
    "    else:\n",
    "        load_features = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    # vectorize before standardization (cause scaler can't do it in that format)\n",
    "    N, ydim, xdim = data.shape\n",
    "    data = data.reshape(N, xdim*ydim)\n",
    "\n",
    "    # standardize\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "\n",
    "    # reshape to original shape\n",
    "    return data.reshape(N, ydim, xdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_channel(data, n_channels=1):\n",
    "    # n_channels: 1 for grey-scale, 3 for RGB, but usually already present in the data\n",
    "    \n",
    "    N, ydim, xdim = data.shape\n",
    "    data = data.reshape(N, ydim, xdim, n_channels)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = standardize(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_channel(data, n_channels=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = data.shape[1:]  \n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Split retains the class balance in both sets\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=testset_size, random_state=0)\n",
    "splits = splitter.split(data, classes)\n",
    "\n",
    "for train_index, test_index in splits:\n",
    "    train_set = data[train_index]\n",
    "    test_set = data[test_index]\n",
    "    train_classes = classes[train_index]\n",
    "    test_classes = classes[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompactCNN(input_shape, nb_conv, nb_filters, n_mels, normalize, nb_hidden, dense_units, \n",
    "               output_shape, activation, dropout, multiple_segments=False, graph_model=False, input_tensor=None):\n",
    "    \n",
    "    melgram_input = Input(shape=input_shape)\n",
    "\n",
    "    if n_mels >= 256:\n",
    "        poolings = [(2, 4), (4, 4), (4, 5), (2, 4), (4, 4)]\n",
    "    elif n_mels >= 128:\n",
    "        poolings = [(2, 4), (4, 4), (2, 5), (2, 4), (4, 4)]\n",
    "    elif n_mels >= 96:\n",
    "        poolings = [(2, 4), (3, 4), (2, 5), (2, 4), (4, 4)]\n",
    "    elif n_mels >= 72:\n",
    "        poolings = [(2, 4), (3, 4), (2, 5), (2, 4), (3, 4)]\n",
    "    elif n_mels >= 64:\n",
    "        poolings = [(2, 4), (2, 4), (2, 5), (2, 4), (4, 4)]\n",
    "\n",
    "    # Determine input axis\n",
    "    if keras.backend.image_dim_ordering() == 'th':\n",
    "        channel_axis = 1\n",
    "        freq_axis = 2\n",
    "        time_axis = 3\n",
    "    else:\n",
    "        channel_axis = 3\n",
    "        freq_axis = 1\n",
    "        time_axis = 2\n",
    "            \n",
    "    # Input block\n",
    "    #x = BatchNormalization(axis=time_axis, name='bn_0_freq')(melgram_input)\n",
    "    \n",
    "    print(freq_axis)\n",
    "        \n",
    "    if normalize == 'batch':\n",
    "        x = BatchNormalization(axis=freq_axis, name='bn_0_freq')(melgram_input)\n",
    "    elif normalize in ('data_sample', 'time', 'freq', 'channel'):\n",
    "        x = Normalization2D(normalize, name='nomalization')(melgram_input)\n",
    "    elif normalize in ('no', 'False'):\n",
    "        x = melgram_input\n",
    "\n",
    "    # Conv block 1\n",
    "    x = Convolution2D(nb_filters[0], (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization(axis=channel_axis, name='bn1')(x)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling2D(pool_size=poolings[0], name='pool1')(x)\n",
    "        \n",
    "    # Conv block 2\n",
    "    x = Convolution2D(nb_filters[1], (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization(axis=channel_axis, name='bn2')(x)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling2D(pool_size=poolings[1], name='pool2')(x)\n",
    "        \n",
    "    # Conv block 3\n",
    "    x = Convolution2D(nb_filters[2], (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization(axis=channel_axis, name='bn3')(x)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling2D(pool_size=poolings[2], name='pool3')(x)\n",
    "    \n",
    "    # Conv block 4\n",
    "    if nb_conv > 3:        \n",
    "        x = Convolution2D(nb_filters[3], (3, 3), padding='same')(x)\n",
    "        x = BatchNormalization(axis=channel_axis, name='bn4')(x)\n",
    "        x = ELU()(x)   \n",
    "        x = MaxPooling2D(pool_size=poolings[3], name='pool4')(x)\n",
    "        \n",
    "    # Conv block 5\n",
    "    if nb_conv == 5:\n",
    "        x = Convolution2D(nb_filters[4], (3, 3), padding='same')(x)\n",
    "        x = BatchNormalization(axis=channel_axis, name='bn5')(x)\n",
    "        x = ELU()(x)\n",
    "        x = MaxPooling2D(pool_size=poolings[4], name='pool5')(x)\n",
    "\n",
    "    # Flatten the outout of the last Conv Layer\n",
    "    x = Flatten()(x)\n",
    "      \n",
    "    if nb_hidden == 1:\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = Dense(dense_units, activation='relu')(x)\n",
    "    elif nb_hidden == 2:\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = Dense(dense_units[0], activation='relu')(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = Dense(dense_units[1], activation='relu')(x) \n",
    "    else:\n",
    "        raise ValueError(\"More than 2 hidden units not supported at the moment.\")\n",
    "    \n",
    "    # Output Layer\n",
    "    x = Dense(output_shape, activation=activation, name = 'output')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(melgram_input, x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of Convolutional Layers\n",
    "nb_conv_layers = 4\n",
    "\n",
    "# number of Filters in each layer\n",
    "nb_filters = [64,64,64,128,128]\n",
    "\n",
    "# number of hidden layers at the end of the model\n",
    "nb_hidden = 1 # 2\n",
    "\n",
    "# how many neurons in each hidden layer\n",
    "dense_units = 128 #[128,56]\n",
    "\n",
    "# how many output units\n",
    "# IN A BINARY CLASSIFICATION TASK with 2 possible outputs, 1 single output unit is sufficent (deciding between 0 and 1)\n",
    "output_shape = 1\n",
    "\n",
    "# which activation function to use for OUTPUT layer\n",
    "# IN A BINARY CLASSIFICATION TASK sigmoid activation is the right choice (activating betwee 0 and 1)\n",
    "output_activation = 'sigmoid'\n",
    "\n",
    "# which type of normalization\n",
    "normalization = 'batch'\n",
    "\n",
    "# droupout\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss for a single label classification task is CATEGORICAL crossentropy\n",
    "loss = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many output units\n",
    "# IN A SINGLE LABEL MULTI-CLASS TASK with N classes, we need N output units\n",
    "output_shape = 5\n",
    "\n",
    "# which activation function to use for OUTPUT layer\n",
    "# IN A SINGLE LABEL MULTI-CLASS TASK with N classes we use softmax activation to BALANCE best between the classes \n",
    "# and find the best decision for ONE class\n",
    "output_activation = 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "adam = optimizers.Adam(lr=0.003, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.01)\n",
    "optimizer = adam\n",
    "\n",
    "batch_size = 32 \n",
    "\n",
    "epochs = 30\n",
    "\n",
    "validation_split=0.1 \n",
    "\n",
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "metrics = ['accuracy', precision, recall]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CompactCNN(input_shape, nb_conv = nb_conv_layers, nb_filters= nb_filters, n_mels = 201, \n",
    "                           normalize=normalization, \n",
    "                           nb_hidden = nb_hidden, dense_units = dense_units, \n",
    "                           output_shape = output_shape, activation = output_activation, \n",
    "                           dropout = dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Summary of Training options\n",
    "\n",
    "print(loss)\n",
    "print(optimizer)\n",
    "print(metrics)\n",
    "print(\"Batch size:\", batch_size, \"Epochs:\", epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# COMPILE MODEL\n",
    "\n",
    "model.compile(loss=loss, metrics=metrics, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TRAINING\n",
    "\n",
    "history = model.fit(data, train_classes, \n",
    "                    #validation_split=validation_split,\n",
    "                     #validation_data=(X_test,y_test), \n",
    "                     epochs=epochs, \n",
    "                     initial_epoch=past_epochs,\n",
    "                     batch_size=batch_size\n",
    "                     )\n",
    "\n",
    "past_epochs += epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute probabilities for the classes (= get outputs of output layer)\n",
    "test_pred_prob = model.predict(test_set)\n",
    "test_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the predicted class, we take the ARG MAX of the row vectors \n",
    "test_pred = np.argmax(test_pred_prob, axis=1)\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for groundtruth\n",
    "test_gt = np.argmax(test_classes, axis=1)\n",
    "test_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get final Accuracy\n",
    "accuracy_score(test_gt, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def cm_analysis(y_true, y_pred, filename, labels, ymap=None, figsize=(10,10)):\n",
    "    \"\"\"\n",
    "    Generate matrix plot of confusion matrix with pretty annotations.\n",
    "    The plot image is saved to disk.\n",
    "    args: \n",
    "      y_true:    true label of the data, with shape (nsamples,)\n",
    "      y_pred:    prediction of the data, with shape (nsamples,)\n",
    "      filename:  filename of figure file to save\n",
    "      labels:    string array, name the order of class labels in the confusion matrix.\n",
    "                 use `clf.classes_` if using scikit-learn models.\n",
    "                 with shape (nclass,).\n",
    "      ymap:      dict: any -> string, length == nclass.\n",
    "                 if not None, map the labels & ys to more understandable strings.\n",
    "                 Caution: original y_true, y_pred and labels must align.\n",
    "      figsize:   the size of the figure plotted.\n",
    "    \"\"\"\n",
    "    if ymap is not None:\n",
    "        y_pred = [ymap[yi] for yi in y_pred]\n",
    "        y_true = [ymap[yi] for yi in y_true]\n",
    "        labels = [ymap[yi] for yi in labels]\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=annot, fmt='', ax=ax)\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.classes_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_analysis(test_gt,test_pred,'1',[0,1,2,3,4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
